{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countInFile(filename, output):\n",
    "    \"\"\" Counts the number of occurences of each word in a text file.\n",
    "        Return a Counter object.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    f = open(filename)\n",
    "    line = f.readline()\n",
    "    i = 0\n",
    "    while line:\n",
    "        for token in word_tokenize(line[1:-5]):\n",
    "            if re.findall(\"[a-zA-Z]\",token):\n",
    "                counter[token] += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"Read file. Line: {:,d}\".format(i),end=\"\\r\",flush=True)\n",
    "        i += 1\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    i = 1\n",
    "    f = open(output, \"w\")\n",
    "    for word, freq in counter.most_common(1000):\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(word,i,freq))\n",
    "        if i % 50 == 0:\n",
    "            print(\"Write file. Line: {}/1000\".format(i),end=\"\\r\",flush=True)\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write file. Line: 1000/1000\r"
     ]
    }
   ],
   "source": [
    "#count the words in train_posts.csv\n",
    "c = countInFile(\"train_posts.csv\", \"freq.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_repetition(token):\n",
    "    \"\"\" this function tests if a token contain a character that is repeted 3 times or more.\n",
    "        if no it returns False\n",
    "        if yes it return a list of tuples of substring of the original token and if it was repeated\n",
    "        For example, for the token \"tomorrrroooooow\" it will returns : \n",
    "            [('tomo', False), ('r', True), ('o', True), ('w', False)]\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = []\n",
    "    i = 0\n",
    "    tmp = ''\n",
    "    contains_repetiton = False\n",
    "    while i < len(token):\n",
    "        if i+1 < len(token) and token[i] == token[i+1] and i+2 < len(token) and token[i] == token[i+2]:\n",
    "            contains_repetiton = True\n",
    "            if tmp :\n",
    "                parsed.append((tmp,False))\n",
    "            c = token[i]\n",
    "            i = i+3\n",
    "            while i < len(token) and token[i] == c:\n",
    "                i += 1\n",
    "            parsed.append((c,True))\n",
    "            tmp = ''\n",
    "        else:\n",
    "            tmp += token[i]\n",
    "            i += 1\n",
    "    if tmp :\n",
    "        parsed.append((tmp, False))\n",
    "    if contains_repetiton :\n",
    "        return parsed\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tomo', False), ('r', True), ('o', True), ('w', False)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_repetition(\"tomorrrroooooow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word(parsed, string = ''):\n",
    "    \"\"\" Returns the list all the different combinations of words from a token previously parsed by `test_repetition` \n",
    "        by reducing the number of consecutive repetition of a character to one or two.\n",
    "        For example, for the token \"tomorrrroooooow\", `test_repetition` parses it into :\n",
    "            [('tomo', False), ('r', True), ('o', True), ('w', False)]\n",
    "        and then, this function returns these combinations : \n",
    "            ['tomorow', 'tomoroow', 'tomorrow', 'tomorroow']\n",
    "    \"\"\"\n",
    "    if not parsed:\n",
    "        return [string]\n",
    "    else :\n",
    "        tmp, is_repeted = parsed.pop(0)\n",
    "        if is_repeted:\n",
    "            return create_word(parsed.copy(), string + tmp) + create_word(parsed.copy(), string + 2 * tmp)\n",
    "        else :\n",
    "            return create_word(parsed.copy(), string + tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tomorow', 'tomoroow', 'tomorrow', 'tomorroow']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_word(test_repetition(\"tomorrrroooooow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cat *.csv | sed 's/ \\r//' | tr -d '\\r' | grep \"^[^ ]*$\" | sort | uniq > ../list_words_en.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the list of English words containing doubles letters\n",
    "fp = open(\"ressources/list_words_en_2.txt\")\n",
    "line = fp.readline()\n",
    "dict_en = set()\n",
    "while line:\n",
    "    dict_en.add(line[:-1])\n",
    "    line = fp.readline()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"sent.1000\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered = []\n",
    "out = open(\"test1000.txt\", \"w\")\n",
    "for l, line in enumerate(test[0].values):\n",
    "    #read the file line by line\n",
    "    tokens = word_tokenize(line)\n",
    "    #tokenize the line using nltk.word_tokenize\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        # then iterate on it\n",
    "        # merge all the consecutive punctuation that has been splited into several tokens\n",
    "        # for example a string like \"!!!\" will be tokenized by nltk.word_tokenize into ['!', '!', '!']\n",
    "        # we merge it into only one token : '!!!'\n",
    "        if token in string.punctuation and i+1 < len(tokens) and tokens[i+1] == token:\n",
    "            t = 1\n",
    "            while i+t+1<len(tokens) and tokens[i+t+1] == token:\n",
    "                t+=1\n",
    "            tokens[i] = 3*token\n",
    "            for j in range(t):\n",
    "                tokens.pop(i+1)\n",
    "    # iterate on it again to aplly some normalization methods on each token\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        if re.search(\"(a+h+){2,}|(h+a+){2,}|^aha$\", tokens[i]): # normalizing the laughs into \"AHAHA\"\n",
    "            tokens[i] = \"AHAH\"\n",
    "            continue\n",
    "        if re.search(\"[a-zA-Z0-9]\\.(com|net|org)(/.*)?\", tokens[i]):\n",
    "            tokens[i] = 'urllink'\n",
    "            continue\n",
    "        if re.search(\"([a-z]+\\.+[a-z]+|\\.+[a-z]+|[a-z]+\\.+)\", tokens[i]):\n",
    "            #all the tokens containing dots are splited around dots\n",
    "            splited =  re.split(\"\\.\", tokens[i])\n",
    "            tmp = splited.pop(0) \n",
    "            while splited and not tmp:\n",
    "                tmp = splited.pop(0)\n",
    "            tokens[i] = tmp\n",
    "            j = i+1\n",
    "            while splited:\n",
    "                tmp = splited.pop(0)\n",
    "                if tmp:\n",
    "                    tokens.insert(j, tmp)\n",
    "                    j += 1\n",
    "            continue\n",
    "        if re.search( \"([0-9]{2}\\/[0-9]{1,2}(\\/[0-9]{2,4})?|[0-9]{1,2}\\/[0-9]{2}(\\/[0-9]{2,4})?)\", tokens[i]):\n",
    "            #We replace all the dates by 'DATE'\n",
    "            tokens[i] = \"DATE\"\n",
    "            continue\n",
    "        if re.search(\"[0-9]{1,2}:[0-9]{1,2}(|am|pm)\", tokens[i]):\n",
    "            tokens[i] = 'DATE'\n",
    "        if re.search(\"([a-z]+\\/+[a-z]+|\\/+[a-z]+|[a-z]+\\/+)\", tokens[i]):\n",
    "            #all the tokens containing '/' are splited around '/'\n",
    "            splited =  re.split(\"\\/\", tokens[i])\n",
    "            tmp = splited.pop(0) \n",
    "            while splited and not tmp:\n",
    "                tmp = splited.pop(0)\n",
    "            tokens[i] = tmp\n",
    "            j = i+1\n",
    "            while splited:\n",
    "                tmp = splited.pop(0)\n",
    "                if tmp:\n",
    "                    tokens.insert(j, tmp)\n",
    "                    j += 1\n",
    "            continue\n",
    "        if re.search(\"([a-z]+\\*+[a-z]+|\\*+[a-z]+|[a-z]+\\*+)\", tokens[i]):\n",
    "            #all the tokens containing '*' are splited around '*'\n",
    "            splited =  re.split(\"\\*\", tokens[i])\n",
    "            tmp = splited.pop(0) \n",
    "            while splited and not tmp:\n",
    "                tmp = splited.pop(0)\n",
    "            tokens[i] = tmp\n",
    "            j = i+1\n",
    "            while splited:\n",
    "                tmp = splited.pop(0)\n",
    "                if tmp:\n",
    "                    tokens.insert(j, tmp)\n",
    "                    j += 1\n",
    "            continue\n",
    "        if re.search(\"[a-z]\", token) and test_repetition(tokens[i]):\n",
    "            #cleaning the words that contains consecutive repetion of a letter 3 times or more\n",
    "            parsed = test_repetition(tokens[i])\n",
    "            words = create_word(parsed)\n",
    "            tokens[i] = words[0]\n",
    "            for word in words[1:]:\n",
    "                if word in dict_en:\n",
    "                    tokens[i] = word\n",
    "                    break\n",
    "            continue\n",
    "        if re.search(\"^(\\-|\\`|<|>|\\*|@)+$\", tokens[i]):\n",
    "            #removing token made of only '-'\n",
    "            tokens.pop(i)\n",
    "            continue\n",
    "        if i>0 and re.search(\"^\\.+$\", tokens[i]) and re.search(\"^\\.+$\", tokens[i-1]): \n",
    "            # if some consecutive token are only dots, then replace them by one token of '...'\n",
    "            tokens[i-1] = \"...\"\n",
    "            tokens.pop(i)\n",
    "            i -= 1\n",
    "            continue\n",
    "        i += 1\n",
    "    filtered.append(' '.join(tokens) )\n",
    "    out.write(' '.join(tokens) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Helloo', '!', '!', '!']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"Helloo!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
