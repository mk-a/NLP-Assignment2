{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk.tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pdsq\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countInFile(filename, output):\n",
    "    \"\"\" Counts the number of occurences of each word in a text file.\n",
    "        Return a Counter object.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    f = open(filename)\n",
    "    line = f.readline()\n",
    "    i = 0\n",
    "    while line:\n",
    "        for token in word_tokenize(line[1:-5]):\n",
    "            if re.findall(\"[a-zA-Z]\",token):\n",
    "                counter[token] += 1\n",
    "        if i % 100 == 0:\n",
    "            print(\"Read file. Line: {:,d}\".format(i),end=\"\\r\",flush=True)\n",
    "        i += 1\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "    i = 1\n",
    "    f = open(output, \"w\")\n",
    "    for word, freq in counter.most_common(1000):\n",
    "        f.write(\"{}\\t{}\\t{}\\n\".format(word,i,freq))\n",
    "        if i % 50 == 0:\n",
    "            print(\"Write file. Line: {}/1000\".format(i),end=\"\\r\",flush=True)\n",
    "        i+=1\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write file. Line: 1000/1000\r"
     ]
    }
   ],
   "source": [
    "#count the words in train_posts.csv\n",
    "c = countInFile(\"train_posts.csv\", \"freq.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_repetition(token):\n",
    "    \"\"\" this function tests if a token contain a character that is repeted 3 times or more.\n",
    "        if no it returns False\n",
    "        if yes it return a list of tuples of substring of the original token and if it was repeated\n",
    "        For example, for the token \"tomorrrroooooow\" it will returns : \n",
    "            [('tomo', False), ('r', True), ('o', True), ('w', False)]\n",
    "    \"\"\"\n",
    "    \n",
    "    parsed = []\n",
    "    i = 0\n",
    "    tmp = ''\n",
    "    contains_repetiton = False\n",
    "    while i < len(token):\n",
    "        if i+1 < len(token) and token[i] == token[i+1] and i+2 < len(token) and token[i] == token[i+2]:\n",
    "            contains_repetiton = True\n",
    "            if tmp :\n",
    "                parsed.append((tmp,False))\n",
    "            c = token[i]\n",
    "            i = i+3\n",
    "            while i < len(token) and token[i] == c:\n",
    "                i += 1\n",
    "            parsed.append((c,True))\n",
    "            tmp = ''\n",
    "        else:\n",
    "            tmp += token[i]\n",
    "            i += 1\n",
    "    if tmp :\n",
    "        parsed.append((tmp, False))\n",
    "    if contains_repetiton :\n",
    "        return parsed\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tomo', False), ('r', True), ('o', True), ('w', False)]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_repetition(\"tomorrrroooooow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word(parsed, string = ''):\n",
    "    \"\"\" Returns the list all the different combinations of words from a token previously parsed by `test_repetition` \n",
    "        by reducing the number of consecutive repetition of a character to one or two.\n",
    "        For example, for the token \"tomorrrroooooow\", `test_repetition` parses it into :\n",
    "            [('tomo', False), ('r', True), ('o', True), ('w', False)]\n",
    "        and then, this function returns these combinations : \n",
    "            ['tomorow', 'tomoroow', 'tomorrow', 'tomorroow']\n",
    "    \"\"\"\n",
    "    if not parsed:\n",
    "        return [string]\n",
    "    else :\n",
    "        tmp, is_repeted = parsed.pop(0)\n",
    "        if is_repeted:\n",
    "            return create_word(parsed.copy(), string + tmp) + create_word(parsed.copy(), string + 2 * tmp)\n",
    "        else :\n",
    "            return create_word(parsed.copy(), string + tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tomorow', 'tomoroow', 'tomorrow', 'tomorroow']"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_word(test_repetition(\"tomorrrroooooow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cat *.csv | sed 's/ \\r//' | tr -d '\\r' | grep \"^[^ ]*$\" | sort | uniq > ../list_words_en.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the list of English words containing doubles letters\n",
    "fp = open(\"ressources/list_words_en_2.txt\")\n",
    "line = fp.readline()\n",
    "dict_en = set()\n",
    "while line:\n",
    "    dict_en.add(line[:-1])\n",
    "    line = fp.readline()\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"sent.1000\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noooo\tno\n",
      "arrrgh\targh\n",
      "grrreat\tgreat\n",
      "waaay\tway\n",
      "waaay\tway\n",
      "arrrgh\targh\n",
      "hmmm\thm\n",
      "ooooh\toh\n",
      "soooo\tso\n",
      "hmmm\thm\n",
      "sooo\tso\n",
      "ohhh\toh\n",
      "sweeeeet\tsweet\tsweet\n",
      "sweeet\tsweet\tsweet\n",
      "ewww\tew\n",
      "sheeesh\tshesh\n",
      "hmmm\thm\n",
      "ooo\too\too\n",
      "hmmm\thm\n",
      "hmmm\thm\n",
      "grrr\tgr\n",
      "dammmo\tdamo\n",
      "yesssssssss\tyes\n",
      "riiiiight\tright\n",
      "tommooooorrroooow\ttommorow\n",
      "willll\twill\twill\n",
      "takkkeeeeee\ttake\n",
      "ussssss\tus\n",
      "awaaaaaaay\taway\n",
      "www.gamefags.com\tw.gamefags.com\n",
      "oooooh\toh\n",
      "www.fictionpress.net/~distantthunder\tw.fictionpress.net/~distantthunder\n",
      "mmmm\tm\n",
      "soooo\tso\n",
      "loooong\tlong\n",
      "soooo\tso\n",
      "oooing\toing\n",
      "helllllooo\thello\thello\n",
      "soooo\tso\n",
      "ummm\tum\n",
      "daaaaaaaaamn\tdamn\n",
      "herrre\there\n",
      "reeeewiiiiinndddd\trewinnd\n",
      "nooo\tno\n",
      ".grrrr\t.gr\n",
      "grrr\tgr\n",
      "grrr\tgr\n",
      "nooooo\tno\n",
      "jacooooozi\tjacozi\n",
      "asss\tass\tass\n",
      "booo\tbo\n",
      "weeeee\twee\twee\n",
      "mmm\tm\n",
      "yephmmm\tyephm\n",
      "allllll\tall\tall\n",
      "looooooooong\tlong\n",
      "goooooooooood\tgood\tgood\n",
      "riiiiight\tright\n",
      "whhhhaaaat\twhat\n",
      "yeeeahhh\tyeah\n",
      "loooooong\tlong\n",
      "hmmm\thm\n",
      "yeeeah\tyeah\n",
      "wellllll\twell\twell\n",
      "www.livejournal.com/cielelric\tw.livejournal.com/cielelric\n",
      "aaaagggghhhhhhhhhhh\tagh\n",
      "tuuuu\ttu\n",
      "peninggg\tpening\n",
      "peninggg\tpening\n",
      "lahhh..\tlah..\n",
      "erghhh..sad\tergh..sad\n",
      "laahhh..aussino\tlaah..aussino\n",
      "cintaaaa..\tcinta..\n",
      "lahhh..\tlah..\n",
      "ergghhhh\terggh\n",
      "blog..mmm..narik\tblog..m..narik\n",
      "soooooooooo\tso\n",
      "heeeeeeeeeeeeee\the\n",
      "like..'harrraaammm\tlike..'haram\n",
      "'..'sadissss..\t'..'sadis..\n",
      "haaaaaa\tha\n",
      "mmm\tm\n",
      "laaa..\tla..\n",
      "sayyy\tsay\n",
      "mmm..\tm..\n",
      "huuu..mintak\thu..mintak\n",
      "aghhhh..\tagh..\n",
      "wooo\twoo\twoo\n",
      "xxx\tx\n",
      "larrr\tlar\n",
      "zzz\tz\n",
      "aaahhh\tah\n",
      "aaahhhhh\tah\n",
      "usss\tus\n",
      "haizzz\thaiz\n",
      "heeee\the\n",
      "heeeee\the\n",
      "heeeee\the\n",
      "haaaa\tha\n",
      "haaaa\tha\n",
      "haaaa\tha\n",
      "zzzzzzzzzzzzzzzzzzzzz..\tz..\n",
      "heee\the\n",
      "hummmmiiinnnngg\thumingg\n",
      "buzzzzbbrrrrgrrrbuzzzz\tbuzbbrgrbuz\n",
      "buzzzzbbrrrrgrrrbuzzzz\tbuzbbrgrbuz\n",
      "yaaaaaawwwnnnnnnn\tyawn\n",
      "www.boortz.com\tw.boortz.com\n",
      "ahhh\tah\n",
      "hmmm\thm\n",
      "eee-ahh-eee-ahh\te-ahh-e-ahh\n",
      "hmmm\thm\n",
      "mmm\tm\n",
      "ssstoppp\tstop\n",
      "laffff\tlaf\n",
      "meeee\tme\n",
      "eowwww\teow\n",
      "whaaatt\twhatt\n",
      "helll\thell\thell\n",
      "ummm\tum\n",
      "hmmm\thm\n",
      "mreeeeeeeeeeeow\tmreow\n",
      "hmmm\thm\n",
      "jooooooooooeeeeeeeeeeeeeelllllllll\tjoel\n",
      "heeeey\they\n",
      "eveeereeebodeee\teverebode\n",
      "reeeow\treow\n",
      "arghhhh\targh\n",
      "yahhhh\tyah\n",
      "haizzzz\thaiz\n",
      "laaaa\tla\n",
      ".arghhhh\t.argh\n",
      "opppss\topss\n",
      "loveee\tlovee\tlovee\n",
      "ooohhh\toh\n",
      "cuuuttteeee\tcute\n",
      "sooo\tso\n",
      "sexxxyyyy\tsexy\n",
      "sooo\tso\n",
      "ooohhh\toh\n",
      "cuuuttteeee\tcute\n",
      "ueeeekk\tuekk\n",
      "coooool\tcool\tcool\n",
      "slipknot.blaaaaah\tslipknot.blah\n",
      "hmmmm\thm\n",
      "soooo\tso\n",
      "happpend\thapend\n",
      "hmmmmm\thm\n",
      "whewwwwwww\twhew\n",
      "dannnnnnnnng\tdang\n",
      "welllll\twell\twell\n",
      "toooooooo\ttoo\ttoo\n",
      "tooo\ttoo\ttoo\n",
      "jkiii/rave\tjki/rave\n",
      "*grrr*\t*gr*\n"
     ]
    }
   ],
   "source": [
    "out = open(\"test1000.txt\", \"w\")\n",
    "for line in test[0].values:\n",
    "    #read the file line by line\n",
    "    tokens = word_tokenize(line)\n",
    "    #tokenize the line using nltk.word_tokenize\n",
    "    for i, token in enumerate(tokens):\n",
    "        # then iterate on it\n",
    "        # merge all the consecutive punctuation that has been splited into several tokens\n",
    "        # for example a string like \"!!!\" will be tokenized by nltk.word_tokenize into ['!', '!', '!']\n",
    "        # we merge it into only one token : '!!!'\n",
    "        if token in string.punctuation and i+1 < len(tokens) and tokens[i+1] == token:\n",
    "            t = 1\n",
    "            while i+t+1<len(tokens) and tokens[i+t+1] == token:\n",
    "                t+=1\n",
    "            tokens[i] = ''.join([token for j in range(t+1)])\n",
    "            for j in range(t):\n",
    "                tokens.pop(i+1)\n",
    "    # iterate on it again to aplly some normalization methods on each token            \n",
    "    for i, token in enumerate(tokens):\n",
    "        if re.search(\"(a+h+){2,}|(h+a+){2,}|^aha$\", token): # normalizing the laughs into \"AHAHA\"\n",
    "            tokens[i] = \"AHAH\"\n",
    "        elif re.search(\"[a-z]\", token) and test_repetition(token):\n",
    "            #cleaning the words that contains consecutive repetion of a letter 3 times or more\n",
    "            print(token,end='\\t') #print the token that contains the repetition\n",
    "            parsed = test_repetition(token)\n",
    "            words = create_word(parsed)\n",
    "            tokens[i] = words[0]\n",
    "            for word in words[1:]:\n",
    "                if word in dict_en:\n",
    "                    print(word, end='\\t')\n",
    "                    tokens[i] = word\n",
    "                    break\n",
    "            print(tokens[i])\n",
    "    out.write(' '.join(tokens) + '\\n')\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
